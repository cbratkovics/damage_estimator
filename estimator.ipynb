{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from uszipcode import SearchEngine, SimpleZipcode, Zipcode\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather housing data from trulia.com based on page URL.\n",
    "def gather_housing_data(url, zipcode):\n",
    "    # Initialize results list to store all data from this page.\n",
    "    results = []\n",
    "    \n",
    "    # Initialize header, request, and beautifulsoup.\n",
    "    header = {'User-Agent':'cjbratkov'}\n",
    "    res = requests.get(url, headers = header)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    # Find and store the price of each house; most important.\n",
    "    house_prices = soup.find_all('span', {'class': 'cardPrice h5 man pan typeEmphasize noWrap typeTruncate'})\n",
    "    prices = []\n",
    "    for i in house_prices:\n",
    "        prices.append(i.text)\n",
    "\n",
    "    # Find and store each house's address.\n",
    "    house_addresses = soup.find_all('div', {'class': 'h6 typeWeightNormal typeTruncate typeLowlight mvn'})\n",
    "    addresses = []\n",
    "    for i in house_addresses:\n",
    "        addresses.append(i.text)\n",
    "\n",
    "    # Find and store the city/state of each house.\n",
    "    house_cities = soup.find_all('div', {'class': 'typeTruncate typeLowlight'})\n",
    "    cities = []\n",
    "    for i in house_cities:\n",
    "        cities.append(i.text)\n",
    "    \n",
    "    # Find and store the bedrooms, bathrooms, and area of each house.\n",
    "    general_house_info = soup.find_all('ul', {'data-testid': 'cardDescription'})\n",
    "    info = []\n",
    "    for g in general_house_info:\n",
    "        s = g.text\n",
    "        s = s.replace(',', '')\n",
    "        info.append(s)\n",
    "\n",
    "    # Save all data for each house in the result dictionary.\n",
    "    for i in range(len(house_prices)):\n",
    "        # Initialize empty result dictionary to store results from each page.\n",
    "        result = {}\n",
    "        \n",
    "        # Store all meaningful values to result dictionary.\n",
    "        result['zipcode'] = zipcode\n",
    "        \n",
    "        # Validate all input.\n",
    "        try: \n",
    "            result['price'] = prices[i]\n",
    "        except IndexError:\n",
    "            result['price'] = np.nan\n",
    "            \n",
    "        try:\n",
    "            result['qualities'] = info[i]\n",
    "        except IndexError:\n",
    "            result['qualities'] = np.nan\n",
    "            \n",
    "        try:\n",
    "            result['address'] = addresses[i]\n",
    "        except IndexError:\n",
    "            result['address'] = np.nan\n",
    "        \n",
    "        try:\n",
    "            result['city/state'] = cities[i]\n",
    "        except IndexError:\n",
    "            result['city/state'] = np.nan\n",
    "    \n",
    "        # Add all result dictionary data to results list.\n",
    "        results.append(result)\n",
    "    \n",
    "    # Return the results from the page and url.\n",
    "    return results, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the URL to the next page.\n",
    "def update_url(page_url):\n",
    "    \n",
    "    # Assume there is no next page.\n",
    "    next_exists = False\n",
    "    \n",
    "    # Initialize header, request, and beautifulsoup.\n",
    "    header = {'User-Agent':'cjbratkov'}\n",
    "    res = requests.get(page_url, headers = header)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "    # Find and store the next page url.\n",
    "    next_pages = (soup.find_all('a', {'class': 'pvl phm'}))\n",
    "    \n",
    "    # If additional pages are found, update the next page URL.\n",
    "    if next_pages:\n",
    "        for p in next_pages:\n",
    "            if (p.attrs['aria-label'] == 'Next page'):\n",
    "                next_page_url = p.attrs['href']\n",
    "                next_exists = True\n",
    "            else:\n",
    "                next_page_url = page_url\n",
    "                \n",
    "    # If next page is not found, assign to current page URL.      \n",
    "    else:\n",
    "        next_page_url = page_url\n",
    "        next_exists = False\n",
    "\n",
    "    # Return the next page's url.\n",
    "    return next_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean scraped data and save it to a temporary data frame.\n",
    "def data_cleaning(df):\n",
    "    # Is the property a studio or not.\n",
    "    df['studio'] = df['qualities'].str.extract(r'(Studio)')\n",
    "    \n",
    "    # Extract number of bathrooms for each property.\n",
    "    df['bath'] = df['qualities'].str.extract(r'([0123456789]ba)')\n",
    "    df['bath'] = df['bath'].map(lambda bath_cell: np.nan if bath_cell == 'NaN' else str(bath_cell))\n",
    "    df['bath'] = df['bath'].map(lambda bath_cell: bath_cell.replace('ba', ''))\n",
    "    df['bath'] = df['bath'].map(lambda bath_cell: bath_cell.replace('0', '10'))\n",
    "    df['bath'] = df['bath'].astype(float)\n",
    "    \n",
    "    # Extract number of bedrooms for each property.\n",
    "    df['bed'] = df['qualities'].str.extract(r'([0123456789]bd)')\n",
    "    df['bed'] = df['bed'].map(lambda bed_cell: np.nan if bed_cell == 'NaN' else str(bed_cell))\n",
    "    df['bed'] = df['bed'].map(lambda bath_cell: bath_cell.replace('bd', ''))\n",
    "    df['bed'] = df['bed'].astype(float)\n",
    "    \n",
    "    # Extract and clean price values for each property.\n",
    "    df['price'] = df['price'].map(lambda price_cell: price_cell.replace('$', ''))\n",
    "    df['price'] = df['price'].map(lambda price_cell: price_cell.replace(',', ''))\n",
    "    df['price'] = df['price'].map(lambda price_cell: price_cell.replace('+', ''))\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    \n",
    "    # Zipcode feature.\n",
    "    df['zipcode'] = df['zipcode'].astype(int)\n",
    "    \n",
    "    # Extract and clean area of each property.\n",
    "    df['drop_sqft'], df['sqft_test'] = df['qualities'].str.split('[0123456789]ba', 1).str\n",
    "    df['sqft_test'] = df['sqft_test'].map(lambda sqft_cell: np.nan if sqft_cell == '' else str(sqft_cell))\n",
    "    df['sqft'], df['sqft_test_2'] = df['sqft_test'].str.split(' ', 1).str\n",
    "    df['sqft'] = df['sqft'].astype(float)\n",
    "    \n",
    "    # Assign assumed values of bedroom and bathroom if the property is a studio.\n",
    "    df.loc[df['studio'] == 'Studio', 'bed'] = 1\n",
    "    df.loc[df['studio'] == 'Studio', 'bath'] = 1\n",
    "    # Feature engineering binary representation of whether the property is a studio or not.\n",
    "    df.loc[df['studio'] != 'Studio', 'studio'] = 0\n",
    "    df.loc[df['studio'] == 'Studio', 'studio'] = 1\n",
    "    df['studio'] = df['studio'].astype(float)\n",
    "    \n",
    "    # Drop unnecessary features.\n",
    "    df.drop(columns= ['drop_sqft', 'sqft_test', 'sqft_test_2', 'qualities'], inplace=True)\n",
    "    \n",
    "    # Return cleaned temporary dataframe.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dictionary class to track the last page to scrape.\n",
    "class last_page_dict(dict):  \n",
    "    def __init__(self):  \n",
    "        self = dict()  \n",
    "    def add_link_with_num(self, link, num):  \n",
    "        self[link] = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver function for the entire process.\n",
    "def driver(zipcode):\n",
    "    # Initialize list of all results.\n",
    "    all_results = []\n",
    "    # Search Engine from uszipcode library to validate zipcode.\n",
    "    search = SearchEngine()\n",
    "    zip_search = search.by_zipcode(zipcode)\n",
    "    base_url = 'https://www.trulia.com/'\n",
    "    \n",
    "    # Gather URL elements if they exist for a zipcode.\n",
    "    try:\n",
    "        city = zip_search.major_city\n",
    "        state = zip_search.state_abbr\n",
    "        city = city.replace(' ', '_')\n",
    "        zcode = zip_search.zipcode\n",
    "        \n",
    "    # If not, skip to next zipcode.\n",
    "    except AttributeError:\n",
    "        return all_results\n",
    "\n",
    "    # Initialize url based on zipcode.\n",
    "    first_url = (base_url + str(state) + '/' + str(city) + '/' + str(zcode) + '/')\n",
    "\n",
    "    # Initialize header, request, and beautifulsoup.\n",
    "    header = {'User-Agent':'cjbratkov'}\n",
    "    res = requests.get(first_url, headers = header)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    # Find and store the last page url.\n",
    "    next_pages = (soup.find_all('a', {'class': 'pvl phm'}))\n",
    "    \n",
    "    link_list = []\n",
    "    link_with_nums = last_page_dict()\n",
    "    num_link = {}\n",
    "    \n",
    "    for p in next_pages:\n",
    "        num_link['text'] = p.text\n",
    "        num_link['attributes'] = p.attrs['href']\n",
    "        link_list.append(p.attrs['href'])\n",
    "\n",
    "    # Extract the last page number and link.\n",
    "    for link in link_list:\n",
    "        num_string_finder = re.search(r'[\\d+]?[\\d+]?[\\d+][_][p][\\/]', link)\n",
    "        num_finder = re.match(r'[\\d+]?[\\d+]?[\\d+]', num_string_finder.group(0))\n",
    "        page_num = num_finder.group(0)\n",
    "        page_num = int(page_num)\n",
    "        link_with_nums.add_link_with_num(link, page_num)\n",
    "    \n",
    "    # Assign defaults to the last page number and last page URL.\n",
    "    last_page_num = 0\n",
    "    last_page_url = first_url\n",
    "    \n",
    "    # Store the last page number and last page URL.\n",
    "    for key, value in link_with_nums.items():\n",
    "        if value > last_page_num:\n",
    "            last_page_num = value\n",
    "            last_page_url = key\n",
    "    \n",
    "    # Gather data from the first zipcode search result.\n",
    "    results, first_page_url = gather_housing_data(first_url, zipcode)\n",
    "    all_results.append(results)\n",
    "    page_url = update_url(first_page_url)\n",
    "    \n",
    "    # Initialize count of scraped pages.\n",
    "    count = 0\n",
    "    \n",
    "    # Gather all data while the last page \n",
    "    while count < last_page_num:\n",
    "        # Gather data from the following pages.\n",
    "        results, next_page_url = gather_housing_data(page_url, zcode)\n",
    "        \n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Update the next page URL after data is gathered from a page.\n",
    "        page_url= update_url(page_url)\n",
    "        count += 1\n",
    "\n",
    "    results, page_url = gather_housing_data(last_page_url, zcode)\n",
    "    all_results.append(results)\n",
    "            \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame builder based on results gathered from trulia.com.\n",
    "def df_builder(all_results):\n",
    "    \n",
    "    # Unpack the nested values from the gathered data.\n",
    "    r = []\n",
    "    for i in all_results:\n",
    "        for j in i:\n",
    "            for n in j:  \n",
    "                r.append(n)\n",
    "          \n",
    "    # Instantiate dataframe.\n",
    "    df = pd.DataFrame(r)\n",
    "    \n",
    "    # Remove duplicate values\n",
    "    df.drop_duplicates(subset = 'address', inplace=True)\n",
    "    \n",
    "    # Return the dataframe.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summary statistics based on gathered data.\n",
    "def summary_stats(df):\n",
    "    # Set float formatting for readability.\n",
    "    pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "    # Initialize list of all summary statistics.\n",
    "    all_stats = []\n",
    "    # Select only numeric columns.\n",
    "    numeric_cols = list(df.select_dtypes(include=['float64']).columns)\n",
    "    # Exclude studio from summary statistics.\n",
    "    df_cols = [c for c in numeric_cols if c != 'studio']\n",
    "    \n",
    "    # For all existing values in each numeric column, generate summary statistics.\n",
    "    for col in df_cols:\n",
    "        # Exclude null values.\n",
    "        stat_col = df[col].dropna()\n",
    "        stats = {}\n",
    "        # Name of feature.\n",
    "        stat_name = col\n",
    "        stats['name'] = col\n",
    "        # Sum\n",
    "        stat_sum = np.sum(stat_col)\n",
    "        stats['sum'] = stat_sum\n",
    "        # Average\n",
    "        stat_mean = np.mean(stat_col)\n",
    "        stats['mean'] = stat_mean\n",
    "        # Minimum\n",
    "        stat_min = np.min(stat_col)\n",
    "        stats['min'] = stat_min\n",
    "        # Maximum\n",
    "        stat_max = np.max(stat_col)\n",
    "        stats['max'] = stat_max\n",
    "        # Median\n",
    "        stat_med = np.median(stat_col)\n",
    "        stats['median'] = stat_med\n",
    "        \n",
    "        # Append all statistics to list for each column.\n",
    "        all_stats.append(stats)\n",
    "       \n",
    "    # Return the summary statistics.\n",
    "    return all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_driver(zipcode_list):\n",
    "    # Initialize empty list of cumulative results.\n",
    "    all_results = []\n",
    "    # Initialize empty list of results per zipcode.\n",
    "    result = []\n",
    "    # Initialize list of invalid zipcodes.\n",
    "    invalid_zips = []\n",
    "    \n",
    "    # Gather data for each zipcode input by the user.\n",
    "    for z in zipcode_list:\n",
    "        result = driver(z)\n",
    "            \n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "        else:\n",
    "            invalid_zips.append(z)\n",
    "    \n",
    "    # Build temporary dataframe of gathered data.\n",
    "    df = df_builder(all_results)\n",
    "    # Clean gathered data.\n",
    "    df = data_cleaning(df)\n",
    "    # Generate summary statistics on cleaned data.\n",
    "    stats = summary_stats(df)\n",
    "    # Delete dataframe.\n",
    "    del(df)\n",
    "    # Return summary statistics and invalid zipcodes.\n",
    "    return stats, invalid_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_zips():\n",
    "    # Set boolean variable to control while loop.\n",
    "    valid_input = True\n",
    "    # Set a dummy string to trigger value error.\n",
    "    s = 'dummy_string'\n",
    "    # Initialize empty list of zipcodes.\n",
    "    zip_list = []\n",
    "    \n",
    "    # Prompt the user for input until no input is given.\n",
    "    while valid_input == True:\n",
    "        print('Enter a zipcode or press enter to get your summary statistics: ')\n",
    "        zipcode = input('--->')\n",
    "        \n",
    "        # Input validation.\n",
    "        try:\n",
    "            if (len(zipcode) == 5):\n",
    "                zipcode = int(zipcode)\n",
    "                \n",
    "                if (zipcode <= 0):\n",
    "                    int(s)\n",
    "            else:\n",
    "                int(s)\n",
    "                \n",
    "            # Append input to list if zipcode is valid.\n",
    "            zip_list.append(zipcode)\n",
    "            \n",
    "        except ValueError:\n",
    "            # If input is empty, execute code.\n",
    "            if zipcode == '':\n",
    "                return zip_list\n",
    "            \n",
    "            print('Value error. Please try again.')\n",
    "            valid_input = True\n",
    "    \n",
    "    # Return list of zipcodes with no duplicates.\n",
    "    return list(set(zip_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drive entire process.\n",
    "def damage_estimate():\n",
    "    # Get the user input list of zipcodes.\n",
    "    zipcode_list = user_zips()\n",
    "    \n",
    "    # Get the invalid zipcodes and summary statistics.\n",
    "    stats, invalid_zips = big_driver(zipcode_list)\n",
    "    \n",
    "    # List of invalid zipcodes.\n",
    "    zipcode_list = [z for z in zipcode_list if z not in invalid_zips]\n",
    "    \n",
    "    # Display invalid zipcodes to user if they exist.\n",
    "    if len(invalid_zips) != 0:\n",
    "        print('Invalid or Unknown Zipcodes: ', invalid_zips)\n",
    "        print()\n",
    "\n",
    "    # Display zipcodes used to generate summary statistics.\n",
    "    print('Estimated Damage Summary Statistics Zipcode Areas : ', zipcode_list)\n",
    "    print()\n",
    "    \n",
    "    # Allow the user to save the statistics as a DataFrame, then CSV. \n",
    "    stats_df = pd.DataFrame.from_dict(stats)\n",
    "    # Display summary statistics.\n",
    "    print(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a zipcode or press enter to get your summary statistics: \n",
      "--->10001\n",
      "Enter a zipcode or press enter to get your summary statistics: \n",
      "--->10011\n",
      "Enter a zipcode or press enter to get your summary statistics: \n",
      "--->10002\n",
      "Enter a zipcode or press enter to get your summary statistics: \n",
      "--->1002a\n",
      "Value error. Please try again.\n",
      "Enter a zipcode or press enter to get your summary statistics: \n",
      "--->12\n",
      "Value error. Please try again.\n",
      "Enter a zipcode or press enter to get your summary statistics: \n",
      "--->10022\n",
      "Enter a zipcode or press enter to get your summary statistics: \n",
      "--->\n",
      "Estimated Damage Summary Statistics Zipcode Areas :  [10001, 10011, 10002, 10022]\n",
      "\n",
      "          max       mean     median      min   name           sum\n",
      "0 67000000.00 3564583.93 1800000.00 62900.00  price 4972594589.00\n",
      "1       10.00       2.32       2.00     1.00   bath       3233.00\n",
      "2        9.00       2.09       2.00     1.00    bed       2912.00\n",
      "3   967300.00   33014.98    1500.00   250.00   sqft   39551947.00\n"
     ]
    }
   ],
   "source": [
    "# testing_zipcode_list = [10011, 10002, 10022, 10001]\n",
    "damage_estimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
