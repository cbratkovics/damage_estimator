**Problem Statement - Extracting Building Values from Trulia**

During a disaster, it is important to model and estimate the potential or forecasted effect of the event, including the projected/forecasted damage. Our code will permit users to input zip codes, and immediately receive data associated with properties within an area, including statistical values aggregated within a neighborhood. Receivable values include last price sold, estimated values, beds, baths, and square feet. Users will be able to immediately find out the sum value, mean value, minimum value, maximum value, and median value for each of these parameters within a zip code. The specific focus of this project is within the 5 boroughs of NYC, and exploring the impact that disasters would have on properties within the 5 boroughs, although the code itself is generalizable to any zip code within the United States on Trulia.

**Navigation - Zip Code Search Engine**

Although the prompt and prior projects put a heavy emphasis on using Zillow to collect real estate data, we found the html nesting of the webpages on Zillow to be extremely unwieldy; it was fairly unfriendly to researchers looking to examine data.  Though we have attached our efforts to scrape our data from Zillow, we have since instead turned to Trulia, whose site contains much more navigable html to work with, with which we can more effectively collect data concerning homes and buildings for our code to run analysis upon.

**Preliminary research**

We were excited to fully explore the project theme, of preparedness and response to emergency disasters. We intended to utilize historical market data circa 3rd and 4th quarter 2011 onwards, to analyze the market impact that Hurricane Sandy had on New York City. Hurricane Sandy bombarded NYC's most flood vulnerable neighborhoods, on the evening of October 29th, 2011, costing well over $50 billion dollars in damage and devastating many homeowners. During preliminary, cursory examinations of historical data on JavaScript line plots available on both Trulia and Zillow, something unexpected became very clear. Despite our recollection of the wreckage this storm caused in the metropolitan area, particularly in South Brooklyn, Queens, and Lower Manhattan, soaring NYC real estate markets were minimally impacted - if you looked at some of the graphs, you would be hard pressed to believe you were looking at the right dates at all!

There are several factors that come into play - people's trust in and appetite for valuable coastal property, insurance and FEMA payouts to affected homeowners and renters, and high rises with building codes conscious of flooding being minimally impacted by the event. The most noticeable devaluations of building markets occurred in residential areas with lots of low rise (less than 4 floor) houses, and even these markets typically recovered to their previous average home values within a year. This all makes it very difficult to interpret what would happen in the event of a more ferocious hurricane or natural disaster impacting the NYC metro area, or another event (likely in the context of climate change) that would raise water levels or otherwise cause severe, more permanent damage.

**Cleaning the Data**

Scraping the data from Trulia gave important features, such as, price, address, city/state and qualities. However, qualities was difficult to read. It included the number of bathrooms, number of bedrooms, square footage and whether or not the address was a studio. To clean the data, we used regular expressions to split and extract the qualities column into multiple columns. After bathrooms, bedrooms, square footage and studio were in their own columns, the next step was to clean the columns. The data types for these columns were all objects. We cleaned the columns and changed the data types to floats. The next step was to make studio a dummy column. One if the address was a studio and zero if the address was not a studio. Finally, most studios did not list the number of bathrooms. We decided to make the assumption that studios only have one bathroom instead of leaving the bathroom column as null.

**US Zip Codes**

A tool we wanted to use was the uszipcode feature from Sanhe Hu. He created a programmable zip code search tool in python (https://uszipcode.readthedocs.io/). The uszipcode tool held data from the 2010 US Census Bureau. Thus it contained housing data. The main feature we wanted to use was the median house value per zip code. We created a function that spit out the median house value for multiple zip codes. The input on the function could have been any city or state. The goal was to do a comparison of the five boroughs. However, the program only had data for Manhattan when we searched New York City. We ultimately decided to not use the uszipcode tool. We decided that it was out of data since the data was from 2010 and there were a good amount of nulls for the median household value. It would have been interesting to compare our search function for Trulia with the 2010 census.

**Implementation**

One of the most important considerations while developing our project was encapsulation of events for ease of use. Our code is written as a series of functions that can ultimately be run on one line of code. Using the BeatifulSoup and requests libraries to scrape housing data from Trulia.com, we were able to consistently gather data based on all valid ZipCodes entered by the user. Trulia provides a clean interface of code that is easily gathered and analyzed. In order to comply with legal regulations, all scraped data is deleted at the end of the process and no data is used or intended use with the goal of commercial gain. Each function used to drive the entire process include prompting the user to enter a list of ZipCodes one at a time, validating each ZipCode entered, searching for the Trulia housing data for each ZipCode, gathering all data from each result page, updating the URL request to scrape data from all search result pages, build a temporary data frame, clean all gathered data, generate summary statistics, and output those summary statistics to the user at the end of the process. Output is available to the user in a simple print display, but it is customizable to be saved as a CSV file for comparison of collateral damage between areas.

**Process Description**

Once a user inputs a list of ZipCodes in the approximate area of potential collateral damage, the process begins by validating the ZipCodes using simple input validation and the uszipcode library. The uszipcode library contains data for each ZipCode and searches uses the state abbreviation and city name to make an initial search request to the first URL search result page on Trulia.com. During this initial request, the last page URL and number are stored using the RegEx library. The page number is used as control flow so the scraper will terminate once it finishes gathering data from the last search result page. In order to gather data from each search result webpage, including house price, number of bedrooms, number of bathrooms, and area of each property is scraped for analysis. Once the first page's data is gathered, the search is updated to a new URL request in order to the next search result page to gather all available data for each ZipCode. Once all search results are gathered for each ZipCode, a temporary data frame is built and cleaned according to the consistent HTML format Trulia has to offer. Once the gathered data is processed, summary statistics are generated using the NumPy library on each data feature. The summary statistics implemented include the minimum, maximum, average, median, and cumulative sum of the gathered features among all the ZipCodes input by the user. Lastly, the summary statistics and their categories are displayed to the user, as well as the valid and invalid ZipCodes entered by the user.

**Visuals**

A feature we will look to add to our search tool is to include visuals when someone searches a zip code or multiple zip codes. We will add bar graphs which compare the statistics with each zip code. Spitting out numbers is great, but visuals can make a bigger difference when looking and trying to interpret the data. One other visual we would add to our search tool is a correlation heat map. It would be interesting to see what features (number of bathrooms, square footage, etc.) have an impact on a zip code especially after a disaster had hit that area. When we used the correlation heat map for our test cases in New York City, we saw that the square footage of the property did not have much of an impact on the market sales price of a home. The number of beds and baths did have a high correlation with the market sales price of a home.

**User Guide**

In order to use this program, the only preparation necessary is updating the header used to make requests to Trulia.com. Once the header is updated, all users must do is enter a list of ZipCodes one at a time. This program can be used to store and/or preview summary statistics estimating the value of one or more areas based on ZipCode. Simple enter the ZipCodes of interest and the program will take care of the rest. If a user makes a mistake when inputting a ZipCode, there is no need to worry. No invalid, duplicate, or nonexistent inputs will interfere with the summary statistics generated for the valid ZipCodes. All invalid or unused ZipCodes, as well as all ZipCodes used in the search will be displayed just before the summary statistics are displayed.

**Conclusions**

With this program, we've developed an easy-to-use tool which can be used to preview, store, and compare summary statistics based upon online housing data. These summary statistics can be used to estimate collateral damage from a variety of disasters using a list of any amount of ZipCodes. The housing data features being analyzed are property price, number of bedrooms, number of bathrooms, and property area. Each feature can be used to estimate dollar value, population, and scope of potential damage caused by disasters. The sum of collateral damage for all properties listed in the ZipCode input can be used to compare cumulative value of all features. The mean can be used to estimate an average damage estimate among all properties. The median can be used in comparison to the mean, which can indicate if there is bias or skew in the data. If the median and mean are significantly different, there could be a causation that is not immediately visible. The minimum and maximum values can be used to better understand the distribution of features, thus accompanying the mean and median.

**Short-Term Future Developments**

Improving the adaptability of our tool is a significant factor that should have the highest priority. The usability of this program is restricted by how significant it is to Trulia.com, because the data gathering process is very specific to the structure of Trulia.com URLs and web pages. Another improvement for the near future would be displaying additional summary statistics for each ZipCode in addition to the cumulative summary statistics for all ZipCodes entered by the user. Even thought users can store the summary statistics and entered ZipCodes manually, improving user experience is always a top priority. 

**Long-Term Future Developments**

**Emergency Condition Time Series Analysis**
If we were able to scrape the JavaScript graphs located here for our historical data - https://www.zillow.com/new-york-ny-11224/home-values/ - our tool would be more effective at demonstrating changes in value over time, and illustrate the difficulty in determining natural disaster impacts on the NYC housing market.  We would also be able to engage in a time series analysis of the home values, sale prices, and listing prices.

**More Comprehensive Data Source**
Neither Trulia, Zillow, or any of the interactive real estate listings and information websites available to the public are comprehensive, and are thus not entirely reflective of housing markets in a given area.  
https://www.census.gov/programs-surveys/acs/data.html ?

**This Project was made in collaboration with General Assembly and New Light Technologies, and was developed by:**
Christopher Bratkovics - cbratkovics@gmail.com
Sean Flanagan – sflanagan94@gmail.com
Eric Liktiger - elikhtiger@gmail.com
